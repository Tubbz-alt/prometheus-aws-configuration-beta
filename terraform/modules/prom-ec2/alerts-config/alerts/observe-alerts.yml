groups:
- name: RE_Observe
  rules:
  - alert: RE_Observe_Grafana_Down
    expr: up{job="grafana-paas"} == 0
    for: 5m
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "Prometheus is not able to scrape Grafana"
        description: "Prometheus has not successfully scraped {{ $labels.job }} in the last 5 minutes. https://grafana-paas.cloudapps.digital/ may be down."
        logs: "https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover?_g=()&_a=(columns:!(_source),index:'*-*',interval:h,query:(query_string:(query:'grafana-paas.cloudapps.digital%20AND%20NOT%20access.response_code:200')),sort:!('@timestamp',desc))"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-grafana-down"

  - alert: RE_Observe_AlertManager_Below_Threshold
    expr: sum(up{job="alertmanager"}) <= 1
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "There is one or less Alertmanagers that can be scraped"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-alertmanager-below-threshold"

  - alert: RE_Observe_Prometheus_Below_Threshold
    expr: sum(up{job="prometheus"}) <= 1
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "There is one or less Prometheis that can be scraped"
        logs: "https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(query:'tags:%20prometheus')),sort:!('@timestamp',desc))"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-prometheus-below-threshold"

  - alert: RE_Observe_PrometheusDiskPredictedToFill
    expr: predict_linear(node_filesystem_avail{ mountpoint="/mnt", job="prometheus_node" }[12h], 3 * 86400)  and on (instance) (time() - node_creation_time) > 43200 <= 0
    labels:
        product: "prometheus"
        severity: "ticket"
    annotations:
        summary: "Instance {{ $labels.instance }} disk {{ $labels.mountpoint }} is predicted to fill in 72h"
        logs: "https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(query:'tags:%20prometheus')),sort:!('@timestamp',desc))"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-prometheus-disk-predicted-to-fill"

  - alert: RE_Observe_No_FileSd_Targets
    # Notes:
    # - this alert will only fire if there are *no* file_sd targets.
    # This is useful if we only have one source of file_sd config, but might not be
    # if we have multiple ways of receiving file_sd configs and only one of them breaks.
    expr: absent(prometheus_sd_file_timestamp)
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "No file_sd targets detected"
        description: "No file_sd targets were detected.  Is there a problem accessing the targets bucket?"
        logs: "https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(query:'tags:%20prometheus')),sort:!('@timestamp',desc))"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-no-filesd-targets"

  - alert: RE_Observe_Prometheus_Over_Capacity
    expr: sum without(slice)(rate(prometheus_engine_query_duration_seconds_sum{job="prometheus"}[5m])) > 8
    for: 10s
    labels:
        product: "prometheus"
        severity: "page"
    annotations:
        summary: "Service is over capacity."
        description: "The service name is {{ $labels.job }}. The URL experiencing the issue is {{ $labels.instance }}."
        logs: "https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(query:'tags:%20prometheus')),sort:!('@timestamp',desc))"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-prometheus-over-capacity"

  - alert: RE_Observe_Prometheus_High_Load
    expr: sum without(slice)(rate(prometheus_engine_query_duration_seconds_sum{job="prometheus"}[2h])) > 4
    labels:
        product: "prometheus"
        severity: "ticket"
    annotations:
        summary: "Service is approaching capacity."
        description: "The service name is {{ $labels.job }}. The URL experiencing the issue is {{ $labels.instance }}."
        logs: "https://kibana.logit.io/s/8fd50110-7b0c-490a-bedf-7544daebbec4/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-15m,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(query:'tags:%20prometheus')),sort:!('@timestamp',desc))"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-prometheus-high-load"

  - alert: RE_Observe_Target_Down
    expr: up{} == 0
    for: 24h
    labels:
        product: "prometheus"
        severity: "ticket"
    annotations:
        summary: "{{ $labels.job }} target is down"
        description: "One of the {{ $labels.job }} targets has been down for 24 hours"
        runbook: "https://re-team-manual.cloudapps.digital/prometheus-for-gds-paas-users.html#re-observe-target-down"

  - alert: AlwaysAlert
    annotations:
      message: |
        This is an alert meant to ensure that the entire alerting pipeline is functional.
        This alert is always firing, therefore it should always be firing in Alertmanager
        and always fire against a receiver.  We use cronitor to alert us if this ever
        *doesn't* fire, because this indicates a problem with our alerting pipeline
    expr: vector(1)
    labels:
        product: "prometheus"
        severity: "constant"
